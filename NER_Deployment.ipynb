{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK2La77aTuYt",
        "outputId": "2ee55764-799c-4efd-881d-7ecaa18d8bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article:\n",
            " At least five people were responsible for the attack, as Mr Hasan said one attacker blew himself up while three others were killed.\r\n",
            "One attacker was captured alive, the police spokesman told reporteâ€¦ [+471 chars]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_news_article(api_key):\n",
        "    url = f'https://newsapi.org/v2/top-headlines?sources=bbc-news&apiKey={api_key}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        articles = response.json().get('articles')\n",
        "        if articles:\n",
        "            return articles[0].get('content')\n",
        "    return None\n",
        "\n",
        "# Replace 'your_api_key' with your actual News API key\n",
        "api_key = '88b7a0a25afd4d85819e8b2f07de2acb'\n",
        "article = fetch_news_article(api_key)\n",
        "print(\"Article:\\n\", article)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "def extract_entities_nltk(text):\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('maxent_ne_chunker')\n",
        "    nltk.download('words')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    entities = []\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        tags = nltk.pos_tag(words)\n",
        "        tree = nltk.ne_chunk(tags, binary=False)\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, nltk.Tree):\n",
        "                entity = \" \".join([word for word, tag in subtree.leaves()])\n",
        "                entity_type = subtree.label()\n",
        "                entities.append((entity, entity_type))\n",
        "    return entities\n",
        "\n",
        "nltk_entities = extract_entities_nltk(article)\n",
        "print(\"Entities from NLTK:\\n\", nltk_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntwD_svhUybz",
        "outputId": "828c987a-2905-499d-89c3-d4365a14cc63"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities from NLTK:\n",
            " [('Hasan', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def extract_entities_spacy(text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "spacy_entities = extract_entities_spacy(article)\n",
        "print(\"Entities from spaCy:\\n\", spacy_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBpPqUInU4Ri",
        "outputId": "41ec635b-b711-4e8e-99d1-88b18a2a5a26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entities from spaCy:\n",
            " [('At least five', 'CARDINAL'), ('Hasan', 'PERSON'), ('one', 'CARDINAL'), ('three', 'CARDINAL'), ('One', 'CARDINAL')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_entities(spacy_entities, nltk_entities):\n",
        "    spacy_set = set(spacy_entities)\n",
        "    nltk_set = set(nltk_entities)\n",
        "\n",
        "    common = spacy_set & nltk_set\n",
        "    spacy_unique = spacy_set - nltk_set\n",
        "    nltk_unique = nltk_set - spacy_set\n",
        "\n",
        "    print(\"Common Entities:\\n\", common)\n",
        "    print(\"\\nEntities unique to spaCy:\\n\", spacy_unique)\n",
        "    print(\"\\nEntities unique to NLTK:\\n\", nltk_unique)\n",
        "\n",
        "compare_entities(spacy_entities, nltk_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY0g37hNU-_P",
        "outputId": "1c41d71c-7c5a-4a78-e898-66538954aaa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Entities:\n",
            " {('Hasan', 'PERSON')}\n",
            "\n",
            "Entities unique to spaCy:\n",
            " {('three', 'CARDINAL'), ('One', 'CARDINAL'), ('one', 'CARDINAL'), ('At least five', 'CARDINAL')}\n",
            "\n",
            "Entities unique to NLTK:\n",
            " set()\n"
          ]
        }
      ]
    }
  ]
}